---
title: "Task 1"
author: "Max Settineri"
date: "2023-02-15"
output: 
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

library(tidyverse)
library(here)
library(broom)
library(patchwork)
library(GGally)
library(ggbeeswarm)
library(tidymodels)
library(AICcmodavg)
library(kableExtra)
```

### Overview

This report examines data on two palmetto species (*Serenoa repens* and *Sabal etonia*) in South Florida to produce a binomial logistic regression model that classifies plant species based on several variables including canopy height, plant length, width, and number of green leaves. First, the relationships between variables and species are visualized to determine which variables might be useful in differentiating between species. Then, the model is evaluated using 10-fold cross validation, AIC, and BIC. Lastly, the model is applied to the data to determine its accuracy in predicting plant species.

**Data source:** Abrahamson, W.G. 2019. Survival, growth and biomass estimates of two dominant palmetto species of south-central Florida from 1981 - 2017, ongoing at 5-year intervals ver 1. Environmental Data Initiative. https://doi.org/10.6073/pasta/f2f96ec76fbbd4b9db431c79a770c4d5

### Reading in and cleaning up the data

```{r}
palmetto <- read_csv(here('data', 'palmetto.csv')) %>% 
  select(height, length, width, green_lvs, species) %>% # selecting variables of interest
  drop_na() %>% # dropping NA values
  mutate(species_name = case_when(
    species == 1 ~ 'Serenoa repens', # new column for species name with full species names
    species == 2 ~ 'Sabal etonia')) %>% 
    mutate(species_name = as_factor(species_name)) # changing species name to a factor for blr
```

### Exploring differences between Serenoa repens and Sabal etonia

```{r, eval = FALSE}
# using GGally to look at trends and relationships between the two species and four variables of interest
palmetto %>% 
  select(species_name, height:green_lvs) %>% 
  ggpairs(aes(color = species_name))

# after initial analysis, looks to be: strong variation in green_lvs by species, slight variation in length and height, minimal difference in width
```

```{r}
# plotting height by species
plot_h <- ggplot(data = palmetto, aes(x = species_name, y = height)) +
  geom_beeswarm(aes(color = species_name)) +
  theme_minimal() +
  labs(x = "Species", y = "Canopy height (cm)") +
  guides(fill = guide_legend(title = "Species")) +
  geom_boxplot(fill = NA, width = 0.4, outlier.color = NA) +
  theme(axis.ticks.x = element_blank(),
    axis.text.x = element_blank()) +
  scale_color_manual(values = c("cyan4", "orange3")) +
  theme(legend.position = c(0.9, 0.8)) +
  theme(legend.background = element_rect(fill="white", 
                                  size=0, linetype="solid"))
```

```{r}
# plotting length by species
plot_l <- ggplot(data = palmetto, aes(x = species_name, y = length)) +
  geom_beeswarm(aes(color = species_name)) +
  theme_minimal() +
  labs(x = "Species", y = "Length (cm)") +
  guides(fill = guide_legend(title = "Species")) +
  geom_boxplot(fill = NA, width = 0.4, outlier.color = NA) +
  theme(axis.ticks.x = element_blank(),
    axis.text.x = element_blank()) +
  scale_color_manual(values = c("cyan4", "orange3")) +
  theme(legend.position = c(0.9, 0.8)) +
  theme(legend.background = element_rect(fill="white", 
                                  size=0, linetype="solid"))
```

```{r}
# plotting width by species
plot_w <- ggplot(data = palmetto, aes(x = species_name, y = width)) +
  geom_beeswarm(aes(color = species_name)) +
  theme_minimal() +
  labs(x = "Species", y = "Width (cm)") +
  guides(fill = guide_legend(title = "Species")) +
  geom_boxplot(fill = NA, width = 0.4, outlier.color = NA) +
  theme(axis.ticks.x = element_blank(),
    axis.text.x = element_blank()) +
  scale_color_manual(values = c("cyan4", "orange3")) +
  theme(legend.position = c(0.9, 0.8)) +
  theme(legend.background = element_rect(fill="white", 
                                  size=0, linetype="solid"))
```

```{r}
# plotting green leaves by species
plot_gl <- ggplot(data = palmetto, aes(x = species_name, y = green_lvs)) +
  geom_beeswarm(aes(color = species_name)) +
  theme_minimal() +
  labs(x = "Species", y = "Number of green leaves") +
  guides(fill = guide_legend(title = "Species")) +
  geom_boxplot(fill = NA, width = 0.4, outlier.color = NA) +
  theme(axis.ticks.x = element_blank(),
    axis.text.x = element_blank()) +
  scale_color_manual(values = c("cyan4", "orange3")) +
  theme(legend.position = c(0.9, 0.8)) +
  theme(legend.background = element_rect(fill="white", 
                                  size=0, linetype="solid"))
```

```{r}
# using patchwork to combine plots
(plot_h | plot_l | plot_w)
#(plot_w | plot_gl)
```

**Figure 1:** 

### Binary logistic regression

Performing a BLR for two models:

- **Model 1:** Log odds of plant type using plant height, canopy length, canopy width and green leaves

- **Model 2:** Log odds of plant type using plant height, canopy width and green leaves

```{r}
# storing functions of combinations of variables for each model
f1 <- species_name ~ height + length + width + green_lvs
f2 <- species_name ~ height + width + green_lvs

# performing and storing the BLR for each model
blr1 <- glm(formula = f1, data = palmetto, family = 'binomial')
blr2 <- glm(formula = f2, data = palmetto, family = 'binomial')

# examining the outputs
#summary(blr1)
#summary(blr2)

blr1_tidy <- tidy(blr1)
blr2_tidy <- tidy(blr2)

blr1_tidy %>% 
  kable(caption = "**Table 1:** Model 1 BLR results",
        col.names = c("Variable", "Coefficient", "Standard error", "Statistic", "p-value")) %>% 
  kable_styling(full_width = FALSE)

blr2_tidy %>% 
  kable(caption = "**Table 2:** Model 2 BLR results",
        col.names = c("Variable", "Coefficient", "Standard error", "Statistic", "p-value")) %>% 
  kable_styling(full_width = FALSE)
```

The coefficients for each predictor variable can be observed in Tables 1 and 2 for our 2 log-linear models. Number of green leaves has the largest coefficient of all variables in both models. 

#### AIC

```{r}
aic <- aictab(list(blr1, blr2))
# model1 = 5194
# model2 = 5987
```

**Model 1:** `r round(aic$AICc[1], 1)`

**Model 2:** `r round(aic$AICc[2], 1)`

The difference in AIC between the two models is large (`r round(aic$Delta_AICc[2], 1)`), showing that model 1 is significantly better fit than model 2.

#### BIC

```{r}
bic <- bictab(list(blr1, blr2))
# model1 = 5232
# model2 = 6017
```

**Model 1:** `r round(bic$BIC[1], 1)`

**Model 2:** `r round(bic$BIC[2], 1)`

The difference in BIC between the two models is large (`r round(bic$Delta_BIC[2], 1)`), showing that model 1 is significantly better fit than model 2.

### Ten fold cross validation 

```{r}
set.seed(218) # setting seed for reproducibility

tidyfold <- vfold_cv(palmetto, v = 10, repeats = 5)

# defining model type 
blr_model <- logistic_reg() %>% 
  set_engine('glm')

# set up workflow that bundles logistic model and formula

# running for formula 1
blr_tidy_wf_1 <- workflow() %>%
  add_model(blr_model) %>%
  add_formula(f1)

blr_tidy_cv_f1 <- blr_tidy_wf_1 %>%
  fit_resamples(tidyfold)

# outputting metrics
cv_metrics_f1 <- collect_metrics(blr_tidy_cv_f1)

# running for formula 2
blr_tidy_wf_2 <- workflow() %>%
  add_model(blr_model) %>%
  add_formula(f2)

blr_tidy_cv_f2 <- blr_tidy_wf_2 %>%
  fit_resamples(tidyfold)

# outputting  metrics
cv_metrics_f2 <- collect_metrics(blr_tidy_cv_f2) 
 
# create tables for cross validation metrics

# model 1
cv_metrics_f1 %>% 
  kable(caption = '**Table 3:** Model 1 Cross Validation',
        col.names = c('Metric', 'Estimator', 'Mean',
                      'n', 'Standard error', 'Configuration')) %>% 
  kable_styling(full_width = FALSE)

# model 2
cv_metrics_f2 %>% 
  kable(caption = '**Table 4:** Model 2 Cross Validation',
        col.names = c('Metric', 'Estimator', 'Mean',
                      'n', 'Standard error', 'Configuration')) %>% 
  kable_styling(full_width = FALSE)
```

As shown in Tables 3 and 4, the mean accuracy of Model 1 is `r round(cv_metrics_f1$mean[1],3)`, while the mean accuracy for Model 2 is `r round(cv_metrics_f2$mean[1],3)`. These cross validation results, combined with the AIC and BIC results, show that Model 1 performs better than Model 2 at species classification.

### Training Model 1 on the entrie data set

```{r}
# using the entire data set to train model 1
blr1_tidyfit <- blr_model %>% 
  fit(f1, data = palmetto)

# output coefficients into a table
coef_blr1_tidy <- tidy(blr1_tidyfit)
coef_blr1_tidy %>% 
  kable(caption = '**Table 5:** Final Model BLR coefficients',
        col.names = c('Variable', 'Coefficient', 'Standard error', 'Statistic', 'p-value')) %>% 
  kable_styling(full_width = FALSE)
```

### Model accuracy

```{r}
# converting log odds to probabilities
blr_fitted <- blr1 %>% 
  broom::augment(type.predict = "response")

# creating a function to calculate accuracy
pred_acc <- function(x, y) {
  accurate <- ifelse(x == y, 0, 1)
  return(accurate)
}

model_success <- blr_fitted %>% 
  mutate(predict_repens = predict(blr1, palmetto, type = "response")) %>% 
  mutate(predicted = ifelse(predict_repens >= 0.50, "Serenoa repens", "Sabal etonia")) %>%
  mutate(accurate = pred_acc(species_name, predicted)) %>% 
  mutate(correct =  case_when(species_name == predicted ~ "Correct",
                              TRUE ~ "Incorrect"))

pred_table <- model_success %>%
  group_by(species_name) %>% 
  summarize(total_accurate = sum(accurate),
            total_inaccurate = n() - sum(accurate),
            model_success = mean(accurate)*100)

pred_table %>% 
  kbl(digits = 2, 
      caption = "**Table 6:** Model Accuracy by Species", 
      col.names = c("Species", "Total Accurate", "Total Inaccurate", "% Correctly Classified")) %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

